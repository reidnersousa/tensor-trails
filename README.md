# ğŸ§  Tensor Trails ğŸš€  
ğŸ“Œ **RepositÃ³rio de Projetos de Deep Learning**  
Este repositÃ³rio contÃ©m projetos semanais para aprimorar habilidades em **VisÃ£o Computacional**, **Processamento de Linguagem Natural (NLP)**, **IA Generativa** e **MLOps/Deploy**.  

---

## ğŸ“… OrganizaÃ§Ã£o dos Projetos  

Cada dia da semana Ã© dedicado a um tema especÃ­fico. Os projetos sÃ£o armazenados em diretÃ³rios separados e documentados com notebooks interativos.  

| Dia da Semana | Tema Principal | Projetos |
|--------------|---------------|----------|
| **Segunda-feira** | ğŸ–¼ï¸ [**VisÃ£o Computacional**](#segunda-feira--visÃ£o-computacional) | ClassificaÃ§Ã£o, DetecÃ§Ã£o e SegmentaÃ§Ã£o de Imagens |
| **TerÃ§a-feira** | ğŸ“ [**Processamento de Linguagem Natural (NLP)**](#terÃ§a-feira--processamento-de-linguagem-natural-nlp) | Fine-tuning, embeddings e LLMs |
| **Quarta-feira** | ğŸš€ [**MLOps & Deploy**](#quinta-feira--mlops--deploy) | Deploy, Docker, MLflow e Kubernetes |


---

## ğŸ“Œ Projetos por Dia  

### **ğŸ“… Segunda-feira | VisÃ£o Computacional**  
ğŸ“‚ [Acesse os projetos](./segunda_visao_computacional/)  

âœ… **NÃ­vel 1: Gender_Classification_CNN**  
ğŸ“Œ Tarefa: Treinar um modelo CNN (ResNet, EfficientNet) para classificar imagens  
- Objetivo: Criar um classificador de imagens usando Transfer Learning.
- Ferramentas: TensorFlow/Keras ou PyTorch, OpenCV, Matplotlib.
- Dataset: [Biggest gender/face recognition dataset.](https://www.kaggle.com/datasets/maciejgronczynski/biggest-genderface-recognition-dataset)
- Entrega: Modelo treinado e um script que recebe uma imagem e retorna a classe prevista.

- ğŸ“‚[Link do projeto](https://www.kaggle.com/code/reidnersantos/gender-classification-cnn-image-dataset)

âœ… **NÃ­vel 2: DetecÃ§Ã£o de Objetos com YOLO/Faster R-CNN**  
ğŸ“Œ Tarefa: Treinar um modelo para detectar objetos em imagens/vÃ­deos 
- Objetivo: Detectar objetos como carros, pessoas, placas, etc.
- Ferramentas: YOLOv8, Faster R-CNN, OpenCV, TensorFlow/PyTorch.
- Dataset: COCO, Pascal VOC, ou dataset customizado.
- Entrega: Script que recebe uma imagem ou vÃ­deo e retorna bounding boxes com classes.
- Desafio Extra: Criar um Streamlit ou Gradio para upload de imagens e exibiÃ§Ã£o da detecÃ§Ã£o.

âœ… **NÃ­vel 3: SegmentaÃ§Ã£o de Imagens (UNet / DeepLabV3)**  
ğŸ“Œ Tarefa: Criar um modelo de segmentaÃ§Ã£o para destacar Ã¡reas especÃ­ficas em imagens  
- Objetivo: Separar objetos do fundo (ex: segmentaÃ§Ã£o de tumores, carros em ruas).
- Ferramentas: UNet, DeepLabV3, TensorFlow/PyTorch, OpenCV.
- Dataset: ISIC (cÃ¢ncer de pele), Cityscapes (segmentaÃ§Ã£o urbana), ou dataset customizado.
- Entrega: Modelo que recebe uma imagem e retorna uma mÃ¡scara segmentada.
- Desafio Extra: Fazer Data Augmentation e comparar o impacto na performance

âœ… **NÃ­vel 4: VisÃ£o Computacional para ProduÃ§Ã£o (Edge AI / Deploy)**  
ğŸ“Œ Tarefa: Rodar um modelo de visÃ£o computacional em um dispositivo embarcado (ex: Raspberry Pi, Jetson Nano)  
 - Objetivo: Criar um sistema de detecÃ§Ã£o/segmentaÃ§Ã£o otimizado para rodar em hardware de baixa potÃªncia.
 - Ferramentas: TensorFlow Lite, OpenVINO, PyTorch Mobile.
 - Entrega: Modelo otimizado rodando em tempo real em um dispositivo embarcado.
 - Desafio Extra: Criar um pipeline de MLOps para treinar e atualizar o modelo remotamente.

---

### **ğŸ“… TerÃ§a-feira | Processamento de Linguagem Natural (NLP)**  
ğŸ“‚ [Acesse os projetos](./terca_nlp/)  

âœ… **NÃ­vel 1: Fundamentos + Fine-Tuning Simples (Para fixar conceitos)**  
ğŸ“Œ Tarefa: ClassificaÃ§Ã£o de sentimentos em tweets (BERT Fine-Tuning)  
- Objetivo: Treinar um modelo BERT para classificar tweets como positivos, negativos ou neutros.
- Ferramentas: Hugging Face Transformers, TensorFlow/PyTorch, Pandas.
- Dataset: Sentiment140.
- Entrega: Criar um script que baixa o dataset, prÃ©-processa os textos, treina o BERT e avalia.
- Desafio Extra: Criar uma API com FastAPI para enviar textos e receber a prediÃ§Ã£o.
- ğŸ“‚[Link do projeto](https://www.kaggle.com/code/reidnersantos/an-lise-de-sentimentos-pytorch)


âœ… **NÃ­vel 2: Embeddings + Busca SemÃ¢ntica**  
ğŸ“Œ Tarefa: CriaÃ§Ã£o de um sistema de busca semÃ¢ntica com embeddings  
- Objetivo: Indexar documentos e usar embeddings para encontrar respostas relevantes.
- Ferramentas: FAISS, ChromaDB, SentenceTransformers (BERT embeddings).
- Dataset: Perguntas e respostas de StackOverflow (ou use seus prÃ³prios textos).
- Entrega: Criar um buscador que recebe uma pergunta e retorna os documentos mais relevantes.
- Desafio Extra: Criar um front-end simples (Streamlit ou Gradio) para interaÃ§Ã£o.

âœ… **NÃ­vel 3: LLMs + RAG (Retrieval-Augmented Generation)**  
ğŸ“Œ Tarefa: ConstruÃ§Ã£o de um Chatbot Inteligente com LangChain + LLMs  
- Objetivo: Criar um chatbot que responde perguntas com base em documentos personalizados.
- Ferramentas: LangChain, OpenAI API / LLaMA / Mistral, ChromaDB (para memÃ³ria).
- Dataset: Artigos cientÃ­ficos, PDFs, notÃ­cias ou base de perguntas sobre deep learning.
- Entrega: Um chatbot que processa PDFs e responde perguntas com base neles.
- Desafio Extra: Deploy com FastAPI e hospedagem no Hugging Face Spaces.

---

### ğŸ“… **Quarta-feira | Trabalhos PrÃ¡ticos de MLOps e Deploy**  
ğŸ“‚ [Acesse os projetos](./quarta_ia_generativa/)  


âœ… **NÃ­vel 1: Deploy Simples de Modelo com FastAPI**
ğŸ“Œ Tarefa: Criar uma API para servir um modelo de classificaÃ§Ã£o de imagens
- Objetivo: Usa a EfficientNet e disponibilizar uma API para receber imagens e retornar previsÃµes.
- Ferramentas: TensorFlow/PyTorch, FastAPI, Uvicorn, Postman para testes.
- Entrega: CÃ³digo que roda localmente e permite enviar imagens via POST request para obter a prediÃ§Ã£o.
- Desafio Extra: Criar uma interface com Streamlit para facilitar o uso
ğŸ“‚[Link do projeto](https://github.com/reidnersousa/Deploy)

âœ… **NÃ­vel 2: DockerizaÃ§Ã£o e Deploy na Nuvem**
ğŸ“Œ Tarefa: Empacotar o modelo dentro de um container Docker e hospedar na nuvem
- Objetivo: Criar um Dockerfile, rodar o modelo dentro de um container e subir para um serviÃ§o na nuvem.
- Ferramentas: Docker, FastAPI, Google Cloud Run / AWS Lambda / Render.
- Entrega: Um endpoint online que recebe uma imagem e retorna a previsÃ£o.
- Desafio Extra: Criar um pipeline CI/CD no GitHub Actions para atualizar o modelo automaticamente.


âœ… **NÃ­vel 3: MLOps Completo com MLflow e Kubernetes**
ğŸ“Œ Tarefa: Criar um pipeline completo de ML, incluindo versionamento e escalabilidade
- Objetivo: Monitorar experimentos com MLflow, salvar modelos e servir via Kubernetes.
- Ferramentas: MLflow, Kubernetes, Terraform (infra as code), Prometheus (monitoramento).
- Entrega: Infraestrutura completa para treinamento, versionamento e deploy do modelo.
- Desafio Extra: Integrar com DVC (Data Version Control) para gerenciar datasets grandes.

---

### **ğŸ“… Quinta-feira | MLOps & Deploy**  
ğŸ“‚ [Acesse os projetos](./quinta_mlops_deploy/)  

âœ… **NÃ­vel 1: Deploy Simples com FastAPI**
ğŸ“Œ Tarefa: Criar uma API para servir um modelo de classificaÃ§Ã£o de imagens
- Carregar um modelo treinado (.h5 ou .pt)
- Criar um endpoint /predict para receber uma imagem e retornar a classe prevista
- Testar a API localmente com curl ou Postman
- SugestÃ£o Extra: Adicionar um endpoint /health para verificar se a API estÃ¡ rodando
- Entrega: Uma API funcional rodando localmente

âœ… **NÃ­vel 2: DockerizaÃ§Ã£o e Deploy na Nuvem**
ğŸ“Œ Tarefa: Colocar a API dentro de um container Docker
- Criar um Dockerfile para empacotar a API
- Testar o container localmente com docker run
- Subir para Google Cloud Run, AWS Lambda ou Render
- SugestÃ£o Extra: Criar um docker-compose.yml para facilitar a execuÃ§Ã£o
- Entrega: API rodando em um serviÃ§o na nuvem


âœ… **NÃ­vel 3: MLOps Completo com MLflow e Kubernetes**
ğŸ“Œ Tarefa: Implementar versionamento de modelos e automaÃ§Ã£o

- Usar MLflow para registrar e versionar os modelos
- Criar um pipeline que automaticamente treina e atualiza o modelo
- Implantar o modelo em um cluster Kubernetes
- SugestÃ£o Extra: Adicionar prometheus para monitoramento
- Entrega: Modelo versionado e em produÃ§Ã£o com controle total


âœ… **NÃ­vel 4: Monitoramento e AutomaÃ§Ã£o**
ğŸ“Œ Tarefa: Criar um pipeline CI/CD para atualizar o modelo automaticamente
- Configurar GitHub Actions para treinar e fazer deploy do modelo
- Adicionar logging e monitoramento (ex: logar previsÃµes em um banco de dados)
- SugestÃ£o Extra: Criar um sistema de feedback loop para re-treinar o modelo com novos dados
- Entrega: Um pipeline completo de ML em produÃ§Ã£o

---

### **Sexta-feira | Docker**



âœ… **NÃ­vel 1: InstalaÃ§Ã£o e ConfiguraÃ§Ã£o BÃ¡sica**
ğŸ“Œ Tarefa: Instalar o Docker e rodar o container "hello-world".
- Objetivo: Familiarizar-se com a instalaÃ§Ã£o do Docker, entender sua arquitetura e executar seu primeiro container para validar a instalaÃ§Ã£o.
- Ferramentas: Docker (Docker Engine), Docker Hub.
- Entrega: Um container "hello-world" em execuÃ§Ã£o, demonstrado pela saÃ­da do comando.
- Desafio Extra: Configurar o Docker para iniciar automaticamente com o sistema e explorar comandos bÃ¡sicos (run, ps, stop).

âœ… **NÃ­vel 2: ContainerizaÃ§Ã£o de uma AplicaÃ§Ã£o Simples**
ğŸ“Œ Tarefa: Desenvolver uma aplicaÃ§Ã£o web simples (por exemplo, usando Flask ou Node.js) e containerizÃ¡-la.

- Objetivo: Criar um Dockerfile que defina o ambiente, copie o cÃ³digo da aplicaÃ§Ã£o e exponha a porta para acesso.
- Ferramentas: Docker, linguagem de programaÃ§Ã£o escolhida (Python/Flask ou Node.js/Express), Dockerfile.
- Entrega: Uma imagem Docker construÃ­da a partir do Dockerfile e um container rodando a aplicaÃ§Ã£o.
- Desafio Extra: Escrever um arquivo Docker Compose para orquestrar a aplicaÃ§Ã£o e facilitar a execuÃ§Ã£o com um Ãºnico comando.


âœ… **NÃ­vel 3: OrquestraÃ§Ã£o de AplicaÃ§Ãµes Multi-Container**
ğŸ“ŒTarefa: Desenvolver uma aplicaÃ§Ã£o composta por mÃºltiplos serviÃ§os (por exemplo, um backend e um banco de dados) utilizando Docker Compose.
- Objetivo: Aprender a definir serviÃ§os, redes e volumes em um arquivo docker-compose.yml, integrando diferentes containers que se comunicam entre si.
- Ferramentas: Docker Compose, Docker, uma aplicaÃ§Ã£o simples (ex.: Flask para o backend e PostgreSQL como banco de dados).
- Entrega: Um projeto funcional onde o comando docker-compose up levanta todos os containers e a aplicaÃ§Ã£o pode ser acessada normalmente.
- Desafio Extra: Configurar variÃ¡veis de ambiente para personalizar as configuraÃ§Ãµes dos serviÃ§os e explorar a escalabilidade replicando um dos serviÃ§os.


âœ… **NÃ­vel 4: Deploy de AplicaÃ§Ãµes em ProduÃ§Ã£o com Pipeline CI/CD**
ğŸ“ŒTarefa: Criar uma pipeline de deploy automatizado para uma aplicaÃ§Ã£o containerizada utilizando Docker e uma ferramenta de CI/CD.
- Objetivo: Implementar um fluxo de integraÃ§Ã£o contÃ­nua e entrega contÃ­nua (CI/CD) que construa, teste e faÃ§a o deploy de uma aplicaÃ§Ã£o em um ambiente de produÃ§Ã£o.
- Ferramentas: Docker, Docker Compose (ou orquestraÃ§Ã£o com Docker Swarm/Kubernetes), FastAPI (para criar a API se necessÃ¡rio), e ferramentas de CI/CD (GitHub Actions, GitLab CI ou similares).
- Entrega: Uma API ou aplicaÃ§Ã£o containerizada que Ã© atualizada automaticamente por meio do pipeline de CI/CD, com documentaÃ§Ã£o do processo.
- Desafio Extra: Implementar monitoramento e logs centralizados (por exemplo, usando Prometheus/Grafana) e configurar um pipeline que tambÃ©m atualize a aplicaÃ§Ã£o automaticamente em caso de novas alteraÃ§Ãµes no repositÃ³rio.

--- 
### **Sabado | Edge Computing**

âœ… **NÃ­vel 1: ConfiguraÃ§Ã£o BÃ¡sica de Dispositivo Edge**
ğŸ“Œ Tarefa: Configurar um Raspberry Pi (ou outro dispositivo similar) para coletar e exibir dados de sensores.

- Objetivo: Implantar um script simples que leia informaÃ§Ãµes (por exemplo, temperatura e umidade) de sensores conectados, processando os dados localmente.
- Ferramentas: Raspberry Pi, Python, bibliotecas de sensores (como Adafruit_GPIO ou RPi.GPIO).
- Dataset: Dados em tempo real coletados pelo sensor durante a operaÃ§Ã£o.
- Entrega: Um script funcional rodando no dispositivo que exiba os dados em um dashboard simples ou via terminal.
- Desafio Extra: Integrar comunicaÃ§Ã£o via MQTT para enviar os dados coletados para um servidor central ou nuvem.

âœ… **NÃ­vel 2: ImplantaÃ§Ã£o de Modelo de InferÃªncia em Edge**
ğŸ“Œ Tarefa: Converter um modelo de Machine Learning treinado (por exemplo, para detecÃ§Ã£o de objetos) para um formato otimizado para dispositivos de borda e implementÃ¡-lo.

- Objetivo: Reduzir a latÃªncia e o consumo de recursos, tornando a inferÃªncia de modelos mais eficiente em um ambiente de Edge Computing.
- Ferramentas: TensorFlow Lite, PyTorch Mobile ou ONNX Runtime; Raspberry Pi ou dispositivo similar.
- Dataset: Utilizar um modelo prÃ©-treinado e, se necessÃ¡rio, ajustar com um conjunto de dados reduzido para testes.
- Entrega: Um dispositivo edge realizando inferÃªncia em tempo real, capaz de processar imagens ou dados de sensores com baixa latÃªncia.
- Desafio Extra: Comparar o desempenho (velocidade e consumo de recursos) entre a inferÃªncia local (edge) e uma soluÃ§Ã£o baseada em nuvem.

âœ… **NÃ­vel 3: Pipeline de Processamento e InferÃªncia com ContÃªineres**
ğŸ“Œ Tarefa: Containerizar uma aplicaÃ§Ã£o de inferÃªncia em Edge e orquestrar o deploy em mÃºltiplos dispositivos.

- Objetivo: Criar um pipeline robusto que integre a coleta de dados, o prÃ©-processamento e a inferÃªncia, facilitando atualizaÃ§Ãµes e escalabilidade.
- Ferramentas: Docker para containerizaÃ§Ã£o, Kubernetes (ou uma versÃ£o leve como K3s) para orquestraÃ§Ã£o, Python.
- Dataset: Dados simulados ou reais coletados de sensores ou cÃ¢meras, conforme o cenÃ¡rio escolhido.
- Entrega: Um pipeline containerizado que pode ser facilmente implantado e atualizado em dispositivos edge, com integraÃ§Ã£o simples via repositÃ³rio (GitHub, por exemplo).
- Desafio Extra: Implementar um mecanismo de autoatualizaÃ§Ã£o ou monitoramento centralizado (usando ferramentas como Prometheus/Grafana) para gerenciar a performance e a saÃºde dos dispositivos.

âœ… **NÃ­vel 4: AplicaÃ§Ã£o Completa - Edge AI em ProduÃ§Ã£o com IntegraÃ§Ã£o CI/CD**
ğŸ“Œ Tarefa: Desenvolver uma API que sirva inferÃªncia de modelos em dispositivos edge e integre um pipeline de CI/CD para atualizaÃ§Ã£o contÃ­nua.

- Objetivo: Unir a capacidade de processamento local com processos de deploy automatizados e monitoramento hÃ­brido (edge + nuvem), garantindo alta disponibilidade e escalabilidade.
- Ferramentas: FastAPI para criar a API, Docker para containerizaÃ§Ã£o, GitHub Actions ou outra ferramenta de CI/CD, e ferramentas de monitoramento (como Prometheus e Grafana).
- Dataset: Utilizar dados reais oriundos de sensores ou de uma aplicaÃ§Ã£o especÃ­fica (ex.: monitoramento de seguranÃ§a ou qualidade de produÃ§Ã£o).
- Entrega: Uma API robusta que receba dados via endpoints, retorne inferÃªncia realizada localmente e registre mÃ©tricas e logs para monitoramento.
- Desafio Extra: Implementar estratÃ©gias de failover e integraÃ§Ã£o com soluÃ§Ãµes na nuvem, permitindo uma abordagem hÃ­brida que garanta redundÃ¢ncia e escalabilidade.


## ğŸ“‚ Estrutura do RepositÃ³rio  

